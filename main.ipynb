{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chensiqi/lhtz/ACMin/main.ipynb 单元格 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloader\u001b[39;00m \u001b[39mimport\u001b[39;00m ClusterData, ClusterLoader\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m cluster_data_save_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/chensiqi/lhtz/ACMin/data/icdm2023_session1_test/processed\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m cluster_data \u001b[39m=\u001b[39m ClusterData(network_data, num_parts\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, save_dir\u001b[39m=\u001b[39mcluster_data_save_dir)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m data_loader \u001b[39m=\u001b[39m ClusterLoader(cluster_data, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network_data' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "def load_data(self):\n",
    "    config = self.config\n",
    "    data_ = dataset_network.get_dataset(config)\n",
    "    if config.dataset == 'icdm_test':\n",
    "        features, edge_index, node_index = data_.data_tensor()\n",
    "        return edge_index, features, node_index\n",
    "    else:\n",
    "        features, true_clusters, edge_index, node_index = data_.data_tensor()\n",
    "        return edge_index, features, true_clusters, node_index\n",
    "cluster_data_save_dir = '/home/chensiqi/lhtz/ACMin/data/icdm2023_session1_test/processed'\n",
    "edge_index, features, node_index = self.load_data()\n",
    "network_data = Data(x=features, edge_index=edge_index, node_index = node_index)\n",
    "cluster_data = ClusterData(network_data, num_parts=10, recursive=True, save_dir=cluster_data_save_dir)\n",
    "data_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True, num_workers=16)\n",
    "for i, batch in enumerate(data_loader):\n",
    "    print(batch.x.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<config.networks.Default object at 0x7fba2290a310>\n",
      "ACMin_cluster_params \t {'alpha': 0.2, 'beta': 0.35, 't': 5, 'tmax': 200, 'ri': False, 'print_batch': 20}\n",
      "batch_size \t 1\n",
      "cluster_parts_num \t 10\n",
      "cluster_result_path \t results\n",
      "data_total_path \t data\n",
      "dataset \t icdm_test\n",
      "mode \t cluster\n",
      "name \t default\n",
      "num_cluster \t 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chensiqi/lhtz/ACMin/dataset_network/base_network.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.x, dtype = torch.float), torch.tensor(edges_df), torch.tensor(self.nodes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_len: 2365188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19389_1185661537_0>: No space left on device (28)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19644_4242872103_0>: No space left on device (28)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 19959) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 19959) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/chensiqi/lhtz/ACMin/main.ipynb 单元格 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39morigin_len: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(available_list)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m node_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mget_index\u001b[39m(ith):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2249444541227d/home/chensiqi/lhtz/ACMin/main.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         nonzero_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(torch\u001b[39m.\u001b[39mall(batch\u001b[39m.\u001b[39mx[ith] \u001b[39m==\u001b[39m features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1296\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1145\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[1;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 19959) exited unexpectedly"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19452_720880470_0>: No space left on device (28)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19516_3724512130_0>: No space left on device (28)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19896_330450989_0>: No space left on device (28)\n",
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000Traceback (most recent call last):\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/chensiqi/anaconda3/envs/py38/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 369, in reduce_storage\n",
      "    fd, size = storage._share_fd_cpu_()\n",
      "RuntimeError: unable to write to file </torch_19707_2794543603_0>: No space left on device (28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "import importlib\n",
    "import torch\n",
    "# from scipy.sparse.linalg.eigen.arpack import eigsh as largest_eigsh\n",
    "# from scipy.sparse.linalg.eigen.arpack import eigs as largest_eigs\n",
    "from scipy.linalg import qr\n",
    "import time\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import dataset_network\n",
    "\n",
    "from config.networks import default as cfg\n",
    "\n",
    "def solve_envs():\n",
    "    mod = importlib.import_module(f'config.networks')\n",
    "    cfg = getattr(mod, 'default')\n",
    "    print(cfg)\n",
    "    # update config\n",
    "    for k, v in cfg.items():\n",
    "            print(k, '\\t', v)\n",
    "    return cfg\n",
    "\n",
    "def load_data(cfg):\n",
    "    config = cfg\n",
    "    data_ = dataset_network.get_dataset(config)\n",
    "    if config.dataset == 'icdm_test':\n",
    "        features, edge_index, node_index = data_.data_tensor()\n",
    "        return edge_index, features, node_index\n",
    "    else:\n",
    "        features, true_clusters, edge_index, node_index = data_.data_tensor()\n",
    "        return edge_index, features, true_clusters, node_index\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cfg = solve_envs()\n",
    "    edge_index, features, node_index = load_data(cfg)\n",
    "    network_data = Data(x=features, edge_index=edge_index, node_index = node_index)\n",
    "    cluster_data_save_dir = 'cluster_data/'\n",
    "    cluster_data = ClusterData(network_data, num_parts=10, recursive=False, save_dir=cluster_data_save_dir)\n",
    "    data_loader = ClusterLoader(cluster_data, batch_size=1, shuffle=True, num_workers=16)\n",
    "    del cluster_data\n",
    "    del network_data\n",
    "    num_batchs = len(data_loader)\n",
    "    available_list = [i for i in range(features.shape[0])]\n",
    "    print(f'origin_len: {len(available_list)}')\n",
    "    node_dict = dict()\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        def get_index(ith):\n",
    "            nonzero_indices = torch.nonzero(torch.all(batch.x[ith] == features, dim=1))\n",
    "            if nonzero_indices.shape[0]>1:\n",
    "                for i_repeat in range(nonzero_indices.shape[0]):\n",
    "                    ind_repeat = nonzero_indices[i_repeat].item()\n",
    "                    if ind_repeat in available_list:\n",
    "                        available_list.remove(ind_repeat)\n",
    "                        return ind_repeat\n",
    "            else:\n",
    "                print(nonzero_indices.item())\n",
    "                print(batch.node_index[ind])\n",
    "                return nonzero_indices.item()       \n",
    "        fs_list = [inde for inde in range(batch.x.shape[0])]\n",
    "        fs_list = fs_list[:10]\n",
    "        for index_fs in fs_list:\n",
    "            get_index(index_fs)\n",
    "    print(f'after_len: {len(available_list)}')\n",
    "    file_path = \"/home/chensiqi/lhtz/ACMin/data/icdm2023_session1_test/processed/node_list.pkl\"\n",
    "    with open(file_path, \"wb\") as file_:\n",
    "        pickle.dump(node_dict, file_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
